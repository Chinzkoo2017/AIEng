{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2EmLIrHaLPR"
      },
      "source": [
        "# üá≤üá≥ –ú–æ–Ω–≥–æ–ª —Ö—ç–ª –¥—ç—ç—Ä Sentiment Analysis Transformer\n",
        "–≠–Ω—ç—Ö“Ø“Ø notebook-–¥ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä –∞—à–∏–≥–ª–∞–Ω  sentiment –∞–Ω–≥–∏–ª–ª—ã–Ω –∑–∞–≥–≤–∞—Ä—ã–≥ PyTorch –¥—ç—ç—Ä —Ö—ç—Ä—ç–≥–∂“Ø“Ø–ª–Ω—ç.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1101RLr7YC2M-leFtDIyVu6BXX1GEUVzS?usp=sharing)"
      ],
      "id": "g2EmLIrHaLPR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1hF4F5QaLPS"
      },
      "outputs": [],
      "source": [
        "# üì¶ –°–∞–Ω–≥—É—É–¥—ã–≥ –∏–º–ø–æ—Ä—Ç–ª–æ—Ö\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "id": "L1hF4F5QaLPS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgKJyzggaLPT"
      },
      "outputs": [],
      "source": [
        "# üìö –ñ–∏—à—ç—ç ”©–≥”©–≥–¥”©–ª ‚Äî 20 ”©–≥“Ø“Ø–ª–±—ç—Ä\n",
        "data = [('–≠–Ω—ç –∫–∏–Ω–æ “Ø–Ω—ç—Ö—ç—ç—Ä –≥–∞–π—Ö–∞–ª—Ç–∞–π –±–∞–π–ª–∞–∞', 1), ('–ë–∏ —ç–Ω—ç –Ω–æ–º–æ–Ω–¥ –º–∞—à –∏—Ö –¥—É—Ä—Ç–∞–π', 1), ('“Æ–Ω—ç—Ö—ç—ç—Ä –º–∞—à –æ–π–ª–≥–æ–º–∂—Ç–æ–π, “Ø—Ä –¥“Ø–Ω—Ç—ç–π –±–∞–π—Å–∞–Ω', 1), ('–≠–Ω—ç —Ä–µ—Å—Ç–æ—Ä–∞–Ω “Ø–Ω—ç—Ö—ç—ç—Ä –∞–º—Ç–≥“Ø–π —Ö–æ–æ–ª—Ç–æ–π', 0), ('“Æ–π–ª—á–∏–ª–≥—ç—ç –º–∞—à –º—É—É –±–∞–π–ª–∞–∞', 0), ('–ö–∏–Ω–æ–Ω—ã “Ø–π–ª —è–≤–¥–∞–ª —É–π—Ç–≥–∞—Ä—Ç–∞–π –±–∞–π—Å–∞–Ω', 0), ('–≠–Ω—ç –≥–∞–∑–∞—Ä –Ω–∞–¥–∞–¥ –º–∞—à –∏—Ö —Ç–∞–∞–ª–∞–≥–¥—Å–∞–Ω', 1), ('–ë–∏ –¥–∞—Ö–∏–∂ —Ö—ç–∑—ç—ç —á —ç–Ω—ç –¥—ç–ª–≥“Ø“Ø—Ä—Ç –æ—Ä–æ—Ö–≥“Ø–π', 0), ('–ì–∞–π—Ö–∞–º—à–∏–≥—Ç–∞–π –±–∞–π–≥–∞–ª—å—Ç–∞–π –≥–∞–∑–∞—Ä –±–∞–π–Ω–∞', 1), ('–ë–∞–≥—à–∏–π–Ω —Ç–∞–π–ª–±–∞—Ä –º–∞—à —Ç–æ–¥–æ—Ä—Ö–æ–π, —Å–∞–π–Ω –±–∞–π—Å–∞–Ω', 1), ('“Æ–Ω—ç—Ç—ç–π –º”©—Ä—Ç–ª”©”© —á–∞–Ω–∞—Ä –º—É—É—Ç–∞–π –±“Ø—Ç—ç—ç–≥–¥—ç—Ö“Ø“Ø–Ω', 0), ('–ë–∏ —ç–Ω—ç –∞–ø–ø–ª–∏–∫–µ–π—à–Ω–∏–π–≥ ”©–¥”©—Ä –±“Ø—Ä –∞—à–∏–≥–ª–∞–¥–∞–≥', 1), ('–≠–Ω—ç –∫–∏–Ω–æ –º–∏–Ω–∏–π —Ü–∞–≥–∏–π–≥ –¥—ç–º–∏–π “Ø—Ä—Å—ç–Ω', 0), ('–ë–∏ —ç–Ω—ç –≥–∞–∑—Ä—ã–Ω —Ö–æ–æ–ª–æ–Ω–¥ –¥—É—Ä—Ç–∞–π', 1), ('–•—ç—Ç—ç—Ä—Ö–∏–π —É–¥–∞–∞–Ω —Ö“Ø—Ä–≥—ç–ª—Ç—Ç—ç–π', 0), ('–ú–∞—à —Å–∞–π–Ω –±–∏—á–∏–≥–¥—Å—ç–Ω –Ω–∏–π—Ç–ª—ç–ª –±–∞–π–Ω–∞', 1), ('–¢–æ–≥–ª–æ–æ–º –±–∞–π–Ω–≥–∞ –∞–ª–¥–∞–∞ ”©–≥–¥”©–≥', 0), ('–ì–æ—ë –¥–∏–∑–∞–π–Ω—Ç–∞–π, —Ö—ç—Ä—ç–≥–ª—ç—Ö—ç–¥ —Ö—è–ª–±–∞—Ä', 1), ('–≠–Ω—ç “Ø–π–ª—á–∏–ª–≥—ç—ç “Ø–Ω—ç—Ö—ç—ç—Ä —Å—ç—Ç–≥—ç–ª –≥–æ–Ω—Å–æ–π–ª–≥–æ—Å–æ–Ω', 0), ('–ù–æ–º—ã–Ω –∞–≥—É—É–ª–≥–∞ –º–∞—à —Å–æ–Ω–∏—Ä—Ö–æ–ª—Ç–æ–π –±–∞–π—Å–∞–Ω', 1)]\n",
        "texts, labels = zip(*data)"
      ],
      "id": "QgKJyzggaLPT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sbvftjZaLPT",
        "outputId": "fd406067-e85a-4c50-fe1a-9f031a6b2927"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "idx2word : {0: '<PAD>', 1: '<UNK>', 2: '—ç–Ω—ç', 3: '–∫–∏–Ω–æ', 4: '“Ø–Ω—ç—Ö—ç—ç—Ä', 5: '–≥–∞–π—Ö–∞–ª—Ç–∞–π', 6: '–±–∞–π–ª–∞–∞', 7: '–±–∏', 8: '–Ω–æ–º–æ–Ω–¥', 9: '–º–∞—à', 10: '–∏—Ö', 11: '–¥—É—Ä—Ç–∞–π', 12: '—Å—É—Ä–≥–∞–ª—Ç', 13: '–æ–π–ª–≥–æ–º–∂—Ç–æ–π', 14: '“Ø—Ä', 15: '–¥“Ø–Ω—Ç—ç–π', 16: '–±–∞–π—Å–∞–Ω', 17: '—Ä–µ—Å—Ç–æ—Ä–∞–Ω', 18: '–∞–º—Ç–≥“Ø–π', 19: '—Ö–æ–æ–ª—Ç–æ–π', 20: '“Ø–π–ª—á–∏–ª–≥—ç—ç', 21: '–º—É—É', 22: '–∫–∏–Ω–æ–Ω—ã', 23: '“Ø–π–ª', 24: '—è–≤–¥–∞–ª', 25: '—É–π—Ç–≥–∞—Ä—Ç–∞–π', 26: '–≥–∞–∑–∞—Ä', 27: '–Ω–∞–¥–∞–¥', 28: '—Ç–∞–∞–ª–∞–≥–¥—Å–∞–Ω', 29: '–¥–∞—Ö–∏–∂', 30: '—Ö—ç–∑—ç—ç', 31: '—á', 32: '–¥—ç–ª–≥“Ø“Ø—Ä—Ç', 33: '–æ—Ä–æ—Ö–≥“Ø–π', 34: '–≥–∞–π—Ö–∞–º—à–∏–≥—Ç–∞–π', 35: '–±–∞–π–≥–∞–ª—å—Ç–∞–π', 36: '–±–∞–π–Ω–∞', 37: '–±–∞–≥—à–∏–π–Ω', 38: '—Ç–∞–π–ª–±–∞—Ä', 39: '—Ç–æ–¥–æ—Ä—Ö–æ–π', 40: '—Å–∞–π–Ω', 41: '“Ø–Ω—ç—Ç—ç–π', 42: '–º”©—Ä—Ç–ª”©”©', 43: '—á–∞–Ω–∞—Ä', 44: '–º—É—É—Ç–∞–π', 45: '–±“Ø—Ç—ç—ç–≥–¥—ç—Ö“Ø“Ø–Ω', 46: '–∞–ø–ø–ª–∏–∫–µ–π—à–Ω–∏–π–≥', 47: '”©–¥”©—Ä', 48: '–±“Ø—Ä', 49: '–∞—à–∏–≥–ª–∞–¥–∞–≥', 50: '–º–∏–Ω–∏–π', 51: '—Ü–∞–≥–∏–π–≥', 52: '–¥—ç–º–∏–π', 53: '“Ø—Ä—Å—ç–Ω', 54: '–≥–∞–∑—Ä—ã–Ω', 55: '—Ö–æ–æ–ª–æ–Ω–¥', 56: '—Ö—ç—Ç—ç—Ä—Ö–∏–π', 57: '—É–¥–∞–∞–Ω', 58: '—Ö“Ø—Ä–≥—ç–ª—Ç—Ç—ç–π', 59: '–±–∏—á–∏–≥–¥—Å—ç–Ω', 60: '–Ω–∏–π—Ç–ª—ç–ª', 61: '—Ç–æ–≥–ª–æ–æ–º', 62: '–±–∞–π–Ω–≥–∞', 63: '–∞–ª–¥–∞–∞', 64: '”©–≥–¥”©–≥', 65: '–≥–æ—ë', 66: '–¥–∏–∑–∞–π–Ω—Ç–∞–π', 67: '—Ö—ç—Ä—ç–≥–ª—ç—Ö—ç–¥', 68: '—Ö—è–ª–±–∞—Ä', 69: '—Å—ç—Ç–≥—ç–ª', 70: '–≥–æ–Ω—Å–æ–π–ª–≥–æ—Å–æ–Ω', 71: '–Ω–æ–º—ã–Ω', 72: '–∞–≥—É—É–ª–≥–∞', 73: '—Å–æ–Ω–∏—Ä—Ö–æ–ª—Ç–æ–π'}\n",
            "vocab_size : 74\n"
          ]
        }
      ],
      "source": [
        "# üî§ Tokenization + Vocabulary “Ø“Ø—Å–≥—ç—Ö\n",
        "from collections import defaultdict\n",
        "import re\n",
        "def tokenize(text):\n",
        "    return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "for text in texts:\n",
        "    for token in tokenize(text):\n",
        "        if token not in word2idx:\n",
        "            word2idx[token] = len(word2idx)\n",
        "idx2word = {i: w for w, i in word2idx.items()}\n",
        "print('idx2word :',idx2word)\n",
        "vocab_size = len(word2idx)\n",
        "print('vocab_size :',vocab_size)"
      ],
      "id": "_sbvftjZaLPT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6ZjK3YtaLPT"
      },
      "outputs": [],
      "source": [
        "# üßæ Text-–∏–π–≥ –∏–Ω–¥–µ–∫—Å –±–æ–ª–≥–æ—Ö\n",
        "max_len = 10\n",
        "def encode(text):\n",
        "    tokens = tokenize(text)\n",
        "    ids = [word2idx.get(t, 1) for t in tokens]\n",
        "    return ids[:max_len] + [0] * (max_len - len(ids))\n",
        "\n",
        "X = torch.tensor([encode(t) for t in texts])\n",
        "y = torch.tensor(labels)"
      ],
      "id": "o6ZjK3YtaLPT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITPHf24xaLPT"
      },
      "outputs": [],
      "source": [
        "# üì¶ Dataset & DataLoader\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, i): return self.X[i], self.y[i]\n",
        "\n",
        "train_ds = SentimentDataset(X, y)\n",
        "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True)"
      ],
      "id": "ITPHf24xaLPT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MfFS0-paLPT",
        "outputId": "48adf700-86a1-43de-b8c6-2e0b43b7696c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# üß† –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –∑–∞–≥–≤–∞—Ä\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=64, nhead=4, num_layers=2, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1, max_len, d_model))\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.classifier = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x) + self.positional_encoding[:, :x.size(1), :]\n",
        "        x = self.transformer(x)\n",
        "        x = x.mean(dim=1)  # pooling\n",
        "        return self.classifier(x)\n",
        "\n",
        "model = TransformerClassifier(vocab_size)"
      ],
      "id": "3MfFS0-paLPT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eycNeYclaLPT"
      },
      "outputs": [],
      "source": [
        "# ‚öôÔ∏è –ê–ª–¥–∞–≥–¥–ª—ã–Ω —Ñ—É–Ω–∫—Ü –±–∞ optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "id": "eycNeYclaLPT"
    },
    {
      "cell_type": "code",
      "source": [
        "# üìä –°—É—Ä–≥–∞–ª—Ç—ã–Ω –±–∞ —Ç–µ—Å—Ç–∏–π–Ω ”©–≥”©–≥–¥”©–ª–¥ —Ö—É–≤–∞–∞—Ö\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "train_ds = SentimentDataset(X_train, y_train)\n",
        "test_ds = SentimentDataset(X_test, y_test)\n",
        "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=4)"
      ],
      "metadata": {
        "id": "sLNPHV81bwxk"
      },
      "id": "sLNPHV81bwxk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üß™ “Æ–Ω—ç–ª–≥—ç—ç —Ö–∏–π—Ö —Ñ—É–Ω–∫—Ü\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            preds = model(xb)\n",
        "            predicted = torch.argmax(preds, dim=1)\n",
        "            correct += (predicted == yb).sum().item()\n",
        "            total += yb.size(0)\n",
        "    acc = correct / total\n",
        "    return acc"
      ],
      "metadata": {
        "id": "2jvaq4zcblV0"
      },
      "id": "2jvaq4zcblV0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JLmWndkaLPT",
        "outputId": "20137352-bb6a-4f23-8beb-2677da1e3196"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss = 0.1716 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 1: Loss = 0.1399 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 2: Loss = 0.1385 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 3: Loss = 0.1215 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 4: Loss = 0.1088 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 5: Loss = 0.1164 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 6: Loss = 0.1229 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 7: Loss = 0.0924 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 8: Loss = 0.0899 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 9: Loss = 0.0938 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 10: Loss = 0.0854 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 11: Loss = 0.0841 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 12: Loss = 0.0905 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 13: Loss = 0.0718 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 14: Loss = 0.0916 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 15: Loss = 0.0708 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 16: Loss = 0.0900 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 17: Loss = 0.0652 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 18: Loss = 0.0618 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 19: Loss = 0.0664 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 20: Loss = 0.0665 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 21: Loss = 0.0573 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 22: Loss = 0.0587 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 23: Loss = 0.0499 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 24: Loss = 0.0614 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 25: Loss = 0.0523 | Train Acc = 1.00, Test Acc = 1.00\n",
            "Epoch 26: Loss = 0.0489 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 27: Loss = 0.0553 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 28: Loss = 0.0498 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 29: Loss = 0.0450 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 30: Loss = 0.0408 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 31: Loss = 0.0415 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 32: Loss = 0.0540 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 33: Loss = 0.0553 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 34: Loss = 0.0458 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 35: Loss = 0.0368 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 36: Loss = 0.0388 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 37: Loss = 0.0411 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 38: Loss = 0.0428 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 39: Loss = 0.0368 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 40: Loss = 0.0391 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 41: Loss = 0.0444 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 42: Loss = 0.0401 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 43: Loss = 0.0450 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 44: Loss = 0.0394 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 45: Loss = 0.0358 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 46: Loss = 0.0267 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 47: Loss = 0.0291 | Train Acc = 1.00, Test Acc = 1.00\n",
            "Epoch 48: Loss = 0.0282 | Train Acc = 1.00, Test Acc = 1.00\n",
            "Epoch 49: Loss = 0.0371 | Train Acc = 1.00, Test Acc = 1.00\n",
            "Epoch 50: Loss = 0.0256 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 51: Loss = 0.0295 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 52: Loss = 0.0257 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 53: Loss = 0.0267 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 54: Loss = 0.0268 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 55: Loss = 0.0256 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 56: Loss = 0.0239 | Train Acc = 1.00, Test Acc = 1.00\n",
            "Epoch 57: Loss = 0.0240 | Train Acc = 1.00, Test Acc = 1.00\n",
            "Epoch 58: Loss = 0.0228 | Train Acc = 1.00, Test Acc = 1.00\n",
            "Epoch 59: Loss = 0.0297 | Train Acc = 1.00, Test Acc = 1.00\n",
            "Epoch 60: Loss = 0.0255 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 61: Loss = 0.0250 | Train Acc = 1.00, Test Acc = 1.00\n",
            "Epoch 62: Loss = 0.0248 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 63: Loss = 0.0230 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 64: Loss = 0.0224 | Train Acc = 1.00, Test Acc = 1.00\n",
            "Epoch 65: Loss = 0.0249 | Train Acc = 1.00, Test Acc = 1.00\n",
            "Epoch 66: Loss = 0.0202 | Train Acc = 1.00, Test Acc = 1.00\n",
            "Epoch 67: Loss = 0.0267 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 68: Loss = 0.0187 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 69: Loss = 0.0196 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 70: Loss = 0.0232 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 71: Loss = 0.0226 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 72: Loss = 0.0217 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 73: Loss = 0.0213 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 74: Loss = 0.0201 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 75: Loss = 0.0185 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 76: Loss = 0.0213 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 77: Loss = 0.0170 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 78: Loss = 0.0188 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 79: Loss = 0.0187 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 80: Loss = 0.0184 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 81: Loss = 0.0172 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 82: Loss = 0.0182 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 83: Loss = 0.0216 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 84: Loss = 0.0177 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 85: Loss = 0.0157 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 86: Loss = 0.0166 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 87: Loss = 0.0163 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 88: Loss = 0.0172 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 89: Loss = 0.0163 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 90: Loss = 0.0152 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 91: Loss = 0.0145 | Train Acc = 1.00, Test Acc = 0.83\n",
            "Epoch 92: Loss = 0.0146 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 93: Loss = 0.0132 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 94: Loss = 0.0120 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 95: Loss = 0.0158 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 96: Loss = 0.0129 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 97: Loss = 0.0122 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 98: Loss = 0.0113 | Train Acc = 1.00, Test Acc = 0.67\n",
            "Epoch 99: Loss = 0.0106 | Train Acc = 1.00, Test Acc = 0.67\n"
          ]
        }
      ],
      "source": [
        "# üèãÔ∏è‚Äç‚ôÇÔ∏è –°—É—Ä–≥–∞–ª—Ç + “Ø–Ω—ç–ª–≥—ç—ç\n",
        "for epoch in range(100):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for xb, yb in train_loader:\n",
        "        preds = model(xb)\n",
        "        loss = criterion(preds, yb)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    train_acc = evaluate(model, train_loader)\n",
        "    test_acc = evaluate(model, test_loader)\n",
        "    print(f\"Epoch {epoch}: Loss = {total_loss:.4f} | Train Acc = {train_acc:.2f}, Test Acc = {test_acc:.2f}\")"
      ],
      "id": "3JLmWndkaLPT"
    },
    {
      "cell_type": "code",
      "source": [
        "# üîÆ Inference ‚Äî ”©–≥“Ø“Ø–ª–±—ç—Ä —Ç–∞–∞–º–∞–≥–ª–∞—Ö —Ñ—É–Ω–∫—Ü\n",
        "def predict_sentiment(text, model):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        encoded = torch.tensor([encode(text)])  # ”©–≥“Ø“Ø–ª–±—ç—Ä–∏–π–≥ encode —Ö–∏–π–Ω—ç\n",
        "        logits = model(encoded)                 # –∑–∞–≥–≤–∞—Ä –¥—ç—ç—Ä –¥–∞–º–∂—É—É–ª–Ω–∞\n",
        "        prediction = torch.argmax(logits, dim=1).item()\n",
        "        return \"–≠–µ—Ä—ç–≥\" if prediction == 1 else \"–°”©—Ä”©–≥\"\n",
        "\n",
        "# üß™ –ñ–∏—à—ç—ç ”©–≥“Ø“Ø–ª–±—ç—Ä“Ø“Ø–¥ –¥—ç—ç—Ä test —Ö–∏–π—Ö:\n",
        "test_sentences = [\n",
        "    \"–≠–Ω—ç –∫–∏–Ω–æ “Ø–Ω—ç—Ö—ç—ç—Ä –≥–æ—ë –±–∞–π–ª–∞–∞\",\n",
        "    \"“Æ–π–ª—á–∏–ª–≥—ç—ç –º–∞—à –º—É—É –±–∞–π—Å–∞–Ω\",\n",
        "    \"–ë–∏ —ç–Ω—ç –∞–ø–ø–ª–∏–∫–µ–π—à–Ω–∏–π–≥ ”©–¥”©—Ä –±“Ø—Ä —Ö—ç—Ä—ç–≥–ª—ç–¥—ç–≥\",\n",
        "    \"–•–æ–æ–ª –Ω—å –∞–º—Ç –º—É—É—Ç–∞–π –±–∞–π–ª–∞–∞\"\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    result = predict_sentiment(sentence, model)\n",
        "    print(f\"'{sentence}' ‚Üí {result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2oJEa1-ba3y",
        "outputId": "fa074f53-1777-405b-85a9-7b6ab820483d"
      },
      "id": "f2oJEa1-ba3y",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'–≠–Ω—ç –∫–∏–Ω–æ “Ø–Ω—ç—Ö—ç—ç—Ä –≥–æ—ë –±–∞–π–ª–∞–∞' ‚Üí –°”©—Ä”©–≥\n",
            "'“Æ–π–ª—á–∏–ª–≥—ç—ç –º–∞—à –º—É—É –±–∞–π—Å–∞–Ω' ‚Üí –°”©—Ä”©–≥\n",
            "'–ë–∏ —ç–Ω—ç –∞–ø–ø–ª–∏–∫–µ–π—à–Ω–∏–π–≥ ”©–¥”©—Ä –±“Ø—Ä —Ö—ç—Ä—ç–≥–ª—ç–¥—ç–≥' ‚Üí –≠–µ—Ä—ç–≥\n",
            "'–•–æ–æ–ª –Ω—å –∞–º—Ç –º—É—É—Ç–∞–π –±–∞–π–ª–∞–∞' ‚Üí –°”©—Ä”©–≥\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}